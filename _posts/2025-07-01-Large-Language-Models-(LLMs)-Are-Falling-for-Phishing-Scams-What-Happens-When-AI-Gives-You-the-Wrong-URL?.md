---
title: Large Language Models (LLMs) Are Falling for Phishing Scams What Happens When AI Gives You the Wrong URL?
date: 2025-07-01
categories: [RESEARCH]
tags: [PHISHING,AI,LANGUAGE MODELS]
---

## Key Data

When Netcraft researchers asked a large language model where to log into various well-known platforms, the results were surprisingly dangerous. Of 131 hostnames provided in response to natural language queries for 50 brands, 34% of them were not controlled by the brands at all.

Two-thirds of the time, the model returned the correct URL. But in the remaining third, the results broke down like this: nearly 30% of the domains were unregistered, parked, or otherwise inactive, leaving them open to takeover. Another 5% pointed users to completely unrelated businesses. In other words, more than one in three users could be sent to a site the brand doesn't own, just by asking a chatbot where to log in.

These were not edge-case prompts. Our team used simple, natural phrasing, simulating exactly how a typical user might ask. The model wasn't trickedâ€”it simply wasn't accurate. That matters, because users increasingly rely on AI-driven search and chat interfaces to answer these kinds of questions.

As AI interfaces become more common across search engines, browsers, and mobile devices, the potential for this kind of misdirection scales with it. The risk is no longer hypothetical.

To read the complete article see:  [Netcraft](https://www.netcraft.com/blog/large-language-models-are-falling-for-phishing-scams)